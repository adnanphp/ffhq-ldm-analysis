\section{Performance Benchmark Analysis}

\subsection{FFHQ Model Benchmark Implementation}

The benchmark script \texttt{ffhq\_benchmark\_fixed.py} performs comprehensive performance testing of the FFHQ latent diffusion model across different batch sizes and measures key metrics including inference speed, memory usage, and generation quality. The benchmark focuses on evaluating the model's practical performance characteristics for real-world deployment scenarios.

\subsubsection{Benchmark Architecture}

\begin{lstlisting}[style=pythonstyle, caption={Benchmark Class Initialization}, label={lst:benchmark_init}]
class FFHQBenchmarkFixed:
    def __init__(self, model_path="models/ldm/ffhq-ldm-vq-4/model.ckpt"):
        """Initialize the benchmark class"""
        print("ğŸ­ FFHQ PERFORMANCE BENCHMARK (FIXED)")
        print("=" * 60)
        
        # Create config
        self.config_path = create_simple_config()
        self.model_path = model_path
        
        # Check for GPU
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ“± Using device: {self.device}")
        
        # Load model with correct configuration
        self.config = OmegaConf.load(self.config_path)
        self.model = instantiate_from_config(self.config.model)
        self.model.eval()
        self.model = self.model.to(self.device)
        
        checkpoint = torch.load(self.model_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['state_dict'], strict=False)
        
        # Create sampler
        self.sampler = DDIMSampler(self.model)
\end{lstlisting}

\subsubsection{Performance Measurement System}

The benchmark implements a systematic approach to measure inference speed across different batch sizes:

\begin{lstlisting}[style=pythonstyle, caption={Performance Measurement Function}, label={lst:benchmark_measure}]
def measure_inference_speed(self, batch_sizes=[1, 2, 4], steps=20, num_repeats=2):
    """Measure inference speed for different batch sizes"""
    
    for batch_size in batch_sizes:
        for repeat in range(num_repeats):
            # Clear memory between runs
            self.clear_memory()
            
            # Generate with timing
            start_time = time.time()
            
            # Generate samples
            samples, _ = self.sampler.sample(
                S=steps,
                conditioning=c,
                batch_size=batch_size,
                shape=shape,
                eta=0.0,
                verbose=False,
                unconditional_guidance_scale=7.5,
                unconditional_conditioning=uc,
            )
            
            sampling_time = time.time() - start_time
            
            # Decode samples to images
            with torch.no_grad():
                x_samples = self.model.decode_first_stage(samples)
                x_samples = torch.clamp((x_samples + 1.0) / 2.0, 0, 1)
            
            total_time = time.time() - start_time
\end{lstlisting}

\subsubsection{Quality Assessment Metrics}

The benchmark includes automatic quality assessment using computer vision heuristics:

\begin{lstlisting}[style=pythonstyle, caption={Face Quality Assessment}, label={lst:benchmark_quality}]
# Simple face detection heuristics
def assess_face_quality(image_np):
    """Assess if generated image resembles a face"""
    contrast = image_np.std()
    brightness = image_np.mean()
    r, g, b = image_np.mean(axis=(0, 1))
    
    # Face-like characteristics
    is_face_like = (contrast > 0.2) and (r > g > b)
    
    return {
        'contrast': contrast,
        'brightness': brightness,
        'is_face_like': is_face_like,
        'color_balance': (r, g, b)
    }
\end{lstlisting}

\subsubsection{Configuration Generation}

The benchmark dynamically creates the correct configuration for the FFHQ model:

\begin{lstlisting}[style=pythonstyle, caption={Configuration Generation}, label={lst:benchmark_config}]
def create_simple_config():
    """Create simple config for unconditional generation"""
    config_yaml = """
model:
  target: ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    scale_factor: 0.18215  # Critical for correct scaling
    conditioning_key: null  # Unconditional generation
    
    unet_config:
      params:
        model_channels: 224  # Must match checkpoint
        num_head_channels: 32  # Critical parameter
        
    first_stage_config:
      params:
        embed_dim: 3  # Latent dimensions
        n_embed: 8192  # Codebook size
    """
\end{lstlisting}

\subsection{Benchmark Methodology}

The benchmark evaluates three key aspects of the FFHQ model:

\subsubsection{1. Inference Speed Analysis}

The benchmark measures:
\begin{itemize}
    \item \textbf{Sampling Time}: Time for diffusion sampling process
    \item \textbf{Decoding Time}: Time to decode latents to images
    \item \textbf{Total Generation Time}: End-to-end image generation time
    \item \textbf{Throughput}: Images generated per second
\end{itemize}

\subsubsection{2. Memory Efficiency}

The benchmark includes:
\begin{itemize}
    \item Memory clearing between runs to ensure fair comparison
    \item GPU memory monitoring (if available)
    \item Batch size scaling analysis
\end{itemize}

\subsubsection{3. Image Quality Assessment}

Quality metrics include:
\begin{itemize}
    \item \textbf{Contrast}: Standard deviation of pixel values
    \item \textbf{Brightness}: Mean pixel value (optimal 0.4-0.6)
    \item \textbf{Color Distribution}: Skin-tone detection (R > G > B)
    \item \textbf{Face Likelihood}: Heuristic assessment of face-like features
\end{itemize}

\subsection{Terminal Output Structure}

The benchmark provides detailed terminal output organized by test phase:

\begin{lstlisting}[style=terminalstyle, caption={Benchmark Terminal Output Structure}]
ğŸ­ FFHQ PERFORMANCE BENCHMARK (FIXED)
============================================================
ğŸ“± Using device: cuda
ğŸ“¦ Loading model...
âœ… Model loaded
ğŸ“ Output directory: benchmark_results_20240115_143022

============================================================
INFERENCE SPEED BENCHMARK
============================================================

ğŸ“Š Settings:
  Steps: 20 (reduced for faster testing)
  Repeats per batch: 2
  Device: cuda
  Batch sizes: [1, 2, 4]
  Output directory: benchmark_results_20240115_143022

========================================
Testing batch size: 1
========================================

  Repeat 1/2...
    âœ… Generated and saved 1 images
    â±ï¸  Total: 2.45s | Sampling: 2.32s | Decoding: 0.13s
    ğŸš€ Speed: 0.41 img/s

========================================
Testing batch size: 2
========================================

  Repeat 1/2...
    âœ… Generated and saved 2 images
    â±ï¸  Total: 4.12s | Sampling: 3.94s | Decoding: 0.18s
    ğŸš€ Speed: 0.49 img/s

========================================
Testing batch size: 4
========================================

  Repeat 1/2...
    âœ… Generated and saved 4 images
    â±ï¸  Total: 7.23s | Sampling: 6.98s | Decoding: 0.25s
    ğŸš€ Speed: 0.55 img/s

âœ… All results saved to: benchmark_results.csv

============================================================
CREATING SAMPLE GRID
============================================================
Generating 4 sample faces...
âœ… Generated 4 faces in 14.23s
ğŸ“ Output files:
  â€¢ Sample grid: sample_grid.png
  â€¢ Individual faces: individual_samples/
  â€¢ Benchmark results: benchmark_results.csv
\end{lstlisting}

\subsection{Key Performance Metrics Collected}

The benchmark records comprehensive metrics in CSV format:

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Metric} & \textbf{Description} \\
\hline
batch\_size & Number of images generated in parallel \\
iteration & Repeat number for statistical validity \\
total\_time & Total generation time (seconds) \\
sampling\_time & Diffusion sampling time (seconds) \\
decoding\_time & Latent decoding time (seconds) \\
images\_per\_sec & Throughput in images per second \\
images\_saved & Number of successfully saved images \\
device & CPU or GPU used for generation \\
steps & Number of diffusion steps \\
guidance\_scale & Classifier-free guidance scale \\
\hline
\end{tabular}
\caption{Benchmark Metrics Recorded}
\label{tab:benchmark_metrics}
\end{table}

\subsection{Visualization Components}

The benchmark generates three types of visual outputs:

\subsubsection{1. Sample Grid}
Generates a grid layout of all generated faces with quality annotations:

\begin{lstlisting}[style=pythonstyle, caption={Grid Generation Code}]
# Create grid layout
fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))

# Add quality annotations
axes[row, col].text(0.02, 0.98, quality,
                   transform=axes[row, col].transAxes,
                   fontsize=9, color=color, fontweight='bold',
                   verticalalignment='top',
                   bbox=dict(boxstyle='round', 
                            facecolor='white', alpha=0.8))
\end{lstlisting}

\subsubsection{2. Performance Plots}
Creates comparative plots showing:
\begin{itemize}
    \item Throughput (images/second) vs batch size
    \item Generation time vs batch size
    \item Efficiency metrics
\end{itemize}

\subsubsection{3. Individual Images}
Saves high-quality individual images for detailed inspection.

\subsection{Benchmark Use Cases}

The benchmark serves multiple purposes:

\subsubsection{1. Hardware Evaluation}
\begin{itemize}
    \item Compare CPU vs GPU performance
    \item Evaluate memory requirements
    \item Test scalability with batch size
\end{itemize}

\subsubsection{2. Model Validation}
\begin{itemize}
    \item Verify correct model loading
    \item Ensure proper face generation
    \item Validate image quality consistency
\end{itemize}

\subsubsection{3. Deployment Planning}
\begin{itemize}
    \item Determine optimal batch sizes
    \item Estimate production throughput
    \item Identify performance bottlenecks
\end{itemize}

\subsubsection{4. Research Comparison}
\begin{itemize}
    \item Baseline performance metrics
    \item Quality assessment standards
    \item Reproducible testing methodology
\end{itemize}

\subsection{Benchmark Limitations}

While comprehensive, the benchmark has limitations:

\begin{itemize}
    \item \textbf{Quality Metrics}: Heuristic-based, not perceptual quality metrics
    \item \textbf{Step Reduction}: Uses reduced steps for speed, affecting quality
    \item \textbf{Hardware Variability}: Results depend on specific hardware
    \item \textbf{Memory Constraints}: May not test extreme batch sizes
    \item \textbf{Single Model}: Only tests the FFHQ model, not generalizable
\end{itemize}

\subsection{Expected Results Structure}

Successful benchmark execution produces:

\begin{verbatim}
benchmark_results_20240115_143022/
â”œâ”€â”€ batch_1/
â”‚   â”œâ”€â”€ batch1_iter0_img0.png
â”‚   â””â”€â”€ batch1_iter1_img0.png
â”œâ”€â”€ batch_2/
â”‚   â”œâ”€â”€ batch2_iter0_img0.png
â”‚   â”œâ”€â”€ batch2_iter0_img1.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ individual_samples/
â”‚   â”œâ”€â”€ face_1.png
â”‚   â”œâ”€â”€ face_2.png
â”‚   â””â”€â”€ ...
â”œâ”€â”€ sample_grid.png
â”œâ”€â”€ performance_plot.png
â””â”€â”€ benchmark_results.csv
\end{verbatim}

The CSV file contains detailed timing data, the PNG files provide visual results, and the directory structure organizes outputs by batch size for easy comparison.

\subsection{Interpretation Guidelines}

When analyzing benchmark results:

\begin{enumerate}
    \item \textbf{Throughput Scaling}: Expect sub-linear scaling with batch size
    \item \textbf{Quality Consistency}: Verify consistent face generation across runs
    \item \textbf{Memory Patterns}: Monitor memory usage patterns
    \item \textbf{Hardware Comparison}: Compare CPU vs GPU performance ratios
    \item \textbf{Quality-Speed Tradeoff}: Analyze how step count affects quality
\end{enumerate}

The benchmark provides a standardized framework for evaluating FFHQ model performance across different deployment scenarios.
